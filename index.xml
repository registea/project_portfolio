<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Science Portfolio on Anthony Registe</title>
    <link>https://registea.github.io/project_portfolio/</link>
    <description>Recent content in Data Science Portfolio on Anthony Registe</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 23 Jul 2020 12:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://registea.github.io/project_portfolio/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Simulating Customer Data</title>
      <link>https://registea.github.io/project_portfolio/post/project-3-generating_synthetic_data/</link>
      <pubDate>Thu, 23 Jul 2020 12:00:00 +0000</pubDate>
      
      <guid>https://registea.github.io/project_portfolio/post/project-3-generating_synthetic_data/</guid>
      <description>Introduction To apply analytics and data science principles, it is necessary to have access to data. There are many places where data is freely available, some examples are:
 Kaggle Wiki Data.gov  There are instances where open source doesnâ€™t meet your precise data needs. In these scenarios, it could be the perfect opportunity to generate synthetic data, which can be built and structured to your exact requirements.
This post is going to use base R with a little help from the tidyverse, to demonstrate how a simple customer dataset can be generated.</description>
    </item>
    
    <item>
      <title>Exploring Tidymodels</title>
      <link>https://registea.github.io/project_portfolio/post/project-1-tidymodels_exploration/</link>
      <pubDate>Mon, 13 Jul 2020 12:00:00 +0000</pubDate>
      
      <guid>https://registea.github.io/project_portfolio/post/project-1-tidymodels_exploration/</guid>
      <description>Introduction The primary objective of this notebook is to explore the tidymodels predictive model framework. I am familiar with the caret package but as Max Kuhn has replaced caret with tidymodels and it has been available for a couple of years, I thought it a good time to take it for a test ride. To enable me to explore the framework a churn dataset provided by Shurti_lyyer on Kaggle will be used.</description>
    </item>
    
    <item>
      <title>Feature Importance: Predictive Power Score</title>
      <link>https://registea.github.io/project_portfolio/post/project-2-predictive_power_score/</link>
      <pubDate>Mon, 13 Jul 2020 12:00:00 +0000</pubDate>
      
      <guid>https://registea.github.io/project_portfolio/post/project-2-predictive_power_score/</guid>
      <description>Introduction This notebook explores the Predictive Power Score (PPS) filter method created by Florian Wetschoreck and posted on Medium. The article describes the PPS as a data type agnostic normalised score of predictive power. The example in the article provided was written in python, this notebook implements the PPS in R, via a custom function.
To explore the PPS, the house price prediction dataset from kaggle is used. This dataset is relatively large from a dimensional perspective but relatively small with regards to observations.</description>
    </item>
    
    <item>
      <title>Contact</title>
      <link>https://registea.github.io/project_portfolio/contact/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://registea.github.io/project_portfolio/contact/</guid>
      <description>Follow me on these social platforms:
   Platform URL     Linkedin: https://www.linkedin.com/in/anthony-registe/   Github: https://github.com/registea   Kaggle: https://www.kaggle.com/ar89dsl    </description>
    </item>
    
  </channel>
</rss>